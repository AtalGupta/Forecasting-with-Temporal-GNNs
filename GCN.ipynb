{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26211f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4946393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prcp1</th>\n",
       "      <th>SM1</th>\n",
       "      <th>Prcp2</th>\n",
       "      <th>SM2</th>\n",
       "      <th>Prcp3</th>\n",
       "      <th>SM3</th>\n",
       "      <th>flood1</th>\n",
       "      <th>flood2</th>\n",
       "      <th>flood3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.539606</td>\n",
       "      <td>0.590441</td>\n",
       "      <td>2.511432</td>\n",
       "      <td>0.474052</td>\n",
       "      <td>4.585283</td>\n",
       "      <td>0.484656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.274125</td>\n",
       "      <td>1.249220</td>\n",
       "      <td>7.461011</td>\n",
       "      <td>2.159259</td>\n",
       "      <td>17.690303</td>\n",
       "      <td>3.474562</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000114</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>1.842560</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3.395878</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.360013</td>\n",
       "      <td>0.429366</td>\n",
       "      <td>2.085287</td>\n",
       "      <td>0.212607</td>\n",
       "      <td>3.610686</td>\n",
       "      <td>0.137334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.158710</td>\n",
       "      <td>0.248820</td>\n",
       "      <td>1.282190</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.592812</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1.357030</td>\n",
       "      <td>0.426691</td>\n",
       "      <td>2.020841</td>\n",
       "      <td>0.197245</td>\n",
       "      <td>3.431358</td>\n",
       "      <td>0.103488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1.154324</td>\n",
       "      <td>0.244887</td>\n",
       "      <td>3.097722</td>\n",
       "      <td>0.201381</td>\n",
       "      <td>6.761527</td>\n",
       "      <td>0.473908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>3.315756</td>\n",
       "      <td>2.183443</td>\n",
       "      <td>2.927501</td>\n",
       "      <td>2.330118</td>\n",
       "      <td>3.422077</td>\n",
       "      <td>2.060617</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1.779925</td>\n",
       "      <td>0.805979</td>\n",
       "      <td>2.532323</td>\n",
       "      <td>0.718383</td>\n",
       "      <td>4.327215</td>\n",
       "      <td>0.680561</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>4.678548</td>\n",
       "      <td>3.405710</td>\n",
       "      <td>6.994089</td>\n",
       "      <td>4.473998</td>\n",
       "      <td>13.186105</td>\n",
       "      <td>5.103808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prcp1       SM1     Prcp2       SM2      Prcp3       SM3  flood1  \\\n",
       "0    1.539606  0.590441  2.511432  0.474052   4.585283  0.484656       0   \n",
       "1    2.274125  1.249220  7.461011  2.159259  17.690303  3.474562       1   \n",
       "2    1.000114  0.106579  1.842560  0.010000   3.395878  0.010000       0   \n",
       "3    1.360013  0.429366  2.085287  0.212607   3.610686  0.137334       0   \n",
       "4    1.158710  0.248820  1.282190  0.010000   1.592812  0.010000       0   \n",
       "..        ...       ...       ...       ...        ...       ...     ...   \n",
       "360  1.357030  0.426691  2.020841  0.197245   3.431358  0.103488       0   \n",
       "361  1.154324  0.244887  3.097722  0.201381   6.761527  0.473908       0   \n",
       "362  3.315756  2.183443  2.927501  2.330118   3.422077  2.060617       1   \n",
       "363  1.779925  0.805979  2.532323  0.718383   4.327215  0.680561       0   \n",
       "364  4.678548  3.405710  6.994089  4.473998  13.186105  5.103808       1   \n",
       "\n",
       "     flood2  flood3  \n",
       "0         0       0  \n",
       "1         1       1  \n",
       "2         0       0  \n",
       "3         0       0  \n",
       "4         0       0  \n",
       "..      ...     ...  \n",
       "360       0       0  \n",
       "361       0       1  \n",
       "362       1       0  \n",
       "363       0       0  \n",
       "364       1       1  \n",
       "\n",
       "[365 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"datasetsrip - synthetic-prcp-SM-flood-data-v1.csv.csv\")\n",
    "data_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb1e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Loss = 0.5853\n",
      "Epoch 2/10: Loss = 0.4378\n",
      "Epoch 3/10: Loss = 0.3817\n",
      "Epoch 4/10: Loss = 0.3510\n",
      "Epoch 5/10: Loss = 0.3135\n",
      "Epoch 6/10: Loss = 0.2790\n",
      "Epoch 7/10: Loss = 0.2543\n",
      "Epoch 8/10: Loss = 0.2416\n",
      "Epoch 9/10: Loss = 0.2143\n",
      "Epoch 10/10: Loss = 0.1991\n",
      "Test Accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "def test_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        # Stack Prcp and SM for each node\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        \n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float), \n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long)\n",
    "            ))\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv1 = GCNConv(in_channels, 16)  \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75 \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_2layer_GCN(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03f2340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Loss = 0.7533\n",
      "Epoch 2/20: Loss = 0.4726\n",
      "Epoch 3/20: Loss = 0.4101\n",
      "Epoch 4/20: Loss = 0.3719\n",
      "Epoch 5/20: Loss = 0.3410\n",
      "Epoch 6/20: Loss = 0.3142\n",
      "Epoch 7/20: Loss = 0.2892\n",
      "Epoch 8/20: Loss = 0.2658\n",
      "Epoch 9/20: Loss = 0.2531\n",
      "Epoch 10/20: Loss = 0.2337\n",
      "Epoch 11/20: Loss = 0.2175\n",
      "Epoch 12/20: Loss = 0.2044\n",
      "Epoch 13/20: Loss = 0.1935\n",
      "Epoch 14/20: Loss = 0.1853\n",
      "Epoch 15/20: Loss = 0.1780\n",
      "Epoch 16/20: Loss = 0.1737\n",
      "Epoch 17/20: Loss = 0.1675\n",
      "Epoch 18/20: Loss = 0.1679\n",
      "Epoch 19/20: Loss = 0.1660\n",
      "Epoch 20/20: Loss = 0.1621\n",
      "Test Accuracy: 0.9194\n"
     ]
    }
   ],
   "source": [
    "def test_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        # Stack Prcp and SM for each node\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        \n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float), \n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long)\n",
    "            ))\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv1 = GCNConv(in_channels, 16)  \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75 \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_2layer_GCN(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f413a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Loss = 1.4361\n",
      "Epoch 2/50: Loss = 0.7346\n",
      "Epoch 3/50: Loss = 0.5307\n",
      "Epoch 4/50: Loss = 0.4375\n",
      "Epoch 5/50: Loss = 0.3869\n",
      "Epoch 6/50: Loss = 0.3518\n",
      "Epoch 7/50: Loss = 0.3225\n",
      "Epoch 8/50: Loss = 0.2996\n",
      "Epoch 9/50: Loss = 0.2810\n",
      "Epoch 10/50: Loss = 0.2616\n",
      "Epoch 11/50: Loss = 0.2450\n",
      "Epoch 12/50: Loss = 0.2282\n",
      "Epoch 13/50: Loss = 0.2192\n",
      "Epoch 14/50: Loss = 0.2077\n",
      "Epoch 15/50: Loss = 0.1942\n",
      "Epoch 16/50: Loss = 0.1905\n",
      "Epoch 17/50: Loss = 0.1875\n",
      "Epoch 18/50: Loss = 0.1746\n",
      "Epoch 19/50: Loss = 0.1823\n",
      "Epoch 20/50: Loss = 0.1714\n",
      "Epoch 21/50: Loss = 0.1645\n",
      "Epoch 22/50: Loss = 0.1631\n",
      "Epoch 23/50: Loss = 0.1640\n",
      "Epoch 24/50: Loss = 0.1662\n",
      "Epoch 25/50: Loss = 0.1581\n",
      "Epoch 26/50: Loss = 0.1578\n",
      "Epoch 27/50: Loss = 0.1556\n",
      "Epoch 28/50: Loss = 0.1543\n",
      "Epoch 29/50: Loss = 0.1613\n",
      "Epoch 30/50: Loss = 0.1476\n",
      "Epoch 31/50: Loss = 0.1490\n",
      "Epoch 32/50: Loss = 0.1484\n",
      "Epoch 33/50: Loss = 0.1538\n",
      "Epoch 34/50: Loss = 0.1513\n",
      "Epoch 35/50: Loss = 0.1508\n",
      "Epoch 36/50: Loss = 0.1461\n",
      "Epoch 37/50: Loss = 0.1461\n",
      "Epoch 38/50: Loss = 0.1468\n",
      "Epoch 39/50: Loss = 0.1539\n",
      "Epoch 40/50: Loss = 0.1633\n",
      "Epoch 41/50: Loss = 0.1654\n",
      "Epoch 42/50: Loss = 0.1478\n",
      "Epoch 43/50: Loss = 0.1469\n",
      "Epoch 44/50: Loss = 0.1465\n",
      "Epoch 45/50: Loss = 0.1466\n",
      "Epoch 46/50: Loss = 0.1448\n",
      "Epoch 47/50: Loss = 0.1452\n",
      "Epoch 48/50: Loss = 0.1419\n",
      "Epoch 49/50: Loss = 0.1416\n",
      "Epoch 50/50: Loss = 0.1408\n",
      "Test Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "def test_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        # Stack Prcp and SM for each node\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        \n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float), \n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long)\n",
    "            ))\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv1 = GCNConv(in_channels, 16)  \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75 \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_2layer_GCN(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be78b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Loss = 0.7920\n",
      "Epoch 2/100: Loss = 0.4956\n",
      "Epoch 3/100: Loss = 0.4264\n",
      "Epoch 4/100: Loss = 0.3731\n",
      "Epoch 5/100: Loss = 0.3471\n",
      "Epoch 6/100: Loss = 0.3233\n",
      "Epoch 7/100: Loss = 0.3046\n",
      "Epoch 8/100: Loss = 0.2715\n",
      "Epoch 9/100: Loss = 0.2518\n",
      "Epoch 10/100: Loss = 0.2371\n",
      "Epoch 11/100: Loss = 0.2205\n",
      "Epoch 12/100: Loss = 0.2114\n",
      "Epoch 13/100: Loss = 0.2132\n",
      "Epoch 14/100: Loss = 0.2033\n",
      "Epoch 15/100: Loss = 0.1834\n",
      "Epoch 16/100: Loss = 0.1722\n",
      "Epoch 17/100: Loss = 0.1686\n",
      "Epoch 18/100: Loss = 0.1628\n",
      "Epoch 19/100: Loss = 0.1643\n",
      "Epoch 20/100: Loss = 0.1626\n",
      "Epoch 21/100: Loss = 0.1550\n",
      "Epoch 22/100: Loss = 0.1558\n",
      "Epoch 23/100: Loss = 0.1594\n",
      "Epoch 24/100: Loss = 0.1517\n",
      "Epoch 25/100: Loss = 0.1469\n",
      "Epoch 26/100: Loss = 0.1394\n",
      "Epoch 27/100: Loss = 0.1387\n",
      "Epoch 28/100: Loss = 0.1367\n",
      "Epoch 29/100: Loss = 0.1359\n",
      "Epoch 30/100: Loss = 0.1304\n",
      "Epoch 31/100: Loss = 0.1301\n",
      "Epoch 32/100: Loss = 0.1337\n",
      "Epoch 33/100: Loss = 0.1325\n",
      "Epoch 34/100: Loss = 0.1289\n",
      "Epoch 35/100: Loss = 0.1240\n",
      "Epoch 36/100: Loss = 0.1211\n",
      "Epoch 37/100: Loss = 0.1217\n",
      "Epoch 38/100: Loss = 0.1226\n",
      "Epoch 39/100: Loss = 0.1246\n",
      "Epoch 40/100: Loss = 0.1260\n",
      "Epoch 41/100: Loss = 0.1153\n",
      "Epoch 42/100: Loss = 0.1129\n",
      "Epoch 43/100: Loss = 0.1115\n",
      "Epoch 44/100: Loss = 0.1106\n",
      "Epoch 45/100: Loss = 0.1093\n",
      "Epoch 46/100: Loss = 0.1102\n",
      "Epoch 47/100: Loss = 0.1101\n",
      "Epoch 48/100: Loss = 0.1085\n",
      "Epoch 49/100: Loss = 0.1056\n",
      "Epoch 50/100: Loss = 0.1030\n",
      "Epoch 51/100: Loss = 0.1017\n",
      "Epoch 52/100: Loss = 0.1050\n",
      "Epoch 53/100: Loss = 0.1012\n",
      "Epoch 54/100: Loss = 0.0981\n",
      "Epoch 55/100: Loss = 0.0968\n",
      "Epoch 56/100: Loss = 0.0968\n",
      "Epoch 57/100: Loss = 0.0973\n",
      "Epoch 58/100: Loss = 0.0963\n",
      "Epoch 59/100: Loss = 0.0979\n",
      "Epoch 60/100: Loss = 0.0898\n",
      "Epoch 61/100: Loss = 0.0871\n",
      "Epoch 62/100: Loss = 0.0865\n",
      "Epoch 63/100: Loss = 0.0858\n",
      "Epoch 64/100: Loss = 0.0925\n",
      "Epoch 65/100: Loss = 0.0906\n",
      "Epoch 66/100: Loss = 0.0818\n",
      "Epoch 67/100: Loss = 0.0810\n",
      "Epoch 68/100: Loss = 0.0765\n",
      "Epoch 69/100: Loss = 0.0790\n",
      "Epoch 70/100: Loss = 0.0749\n",
      "Epoch 71/100: Loss = 0.0795\n",
      "Epoch 72/100: Loss = 0.0768\n",
      "Epoch 73/100: Loss = 0.0705\n",
      "Epoch 74/100: Loss = 0.0733\n",
      "Epoch 75/100: Loss = 0.0723\n",
      "Epoch 76/100: Loss = 0.0696\n",
      "Epoch 77/100: Loss = 0.0682\n",
      "Epoch 78/100: Loss = 0.0656\n",
      "Epoch 79/100: Loss = 0.0675\n",
      "Epoch 80/100: Loss = 0.0647\n",
      "Epoch 81/100: Loss = 0.0677\n",
      "Epoch 82/100: Loss = 0.0662\n",
      "Epoch 83/100: Loss = 0.0797\n",
      "Epoch 84/100: Loss = 0.0696\n",
      "Epoch 85/100: Loss = 0.0655\n",
      "Epoch 86/100: Loss = 0.0619\n",
      "Epoch 87/100: Loss = 0.0648\n",
      "Epoch 88/100: Loss = 0.0656\n",
      "Epoch 89/100: Loss = 0.0654\n",
      "Epoch 90/100: Loss = 0.0786\n",
      "Epoch 91/100: Loss = 0.0651\n",
      "Epoch 92/100: Loss = 0.0597\n",
      "Epoch 93/100: Loss = 0.0554\n",
      "Epoch 94/100: Loss = 0.0564\n",
      "Epoch 95/100: Loss = 0.0554\n",
      "Epoch 96/100: Loss = 0.0560\n",
      "Epoch 97/100: Loss = 0.0551\n",
      "Epoch 98/100: Loss = 0.0562\n",
      "Epoch 99/100: Loss = 0.0541\n",
      "Epoch 100/100: Loss = 0.0554\n",
      "Test Accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "def test_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        # Stack Prcp and SM for each node\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        \n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float), \n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long)\n",
    "            ))\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv1 = GCNConv(in_channels, 16)  \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75 \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_2layer_GCN(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd842a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
