{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a9e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric_temporal.signal import temporal_signal_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82a1319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prcp1</th>\n",
       "      <th>SM1</th>\n",
       "      <th>Prcp2</th>\n",
       "      <th>SM2</th>\n",
       "      <th>Prcp3</th>\n",
       "      <th>SM3</th>\n",
       "      <th>flood1</th>\n",
       "      <th>flood2</th>\n",
       "      <th>flood3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.539606</td>\n",
       "      <td>0.590441</td>\n",
       "      <td>2.511432</td>\n",
       "      <td>0.474052</td>\n",
       "      <td>4.585283</td>\n",
       "      <td>0.484656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.274125</td>\n",
       "      <td>1.249220</td>\n",
       "      <td>7.461011</td>\n",
       "      <td>2.159259</td>\n",
       "      <td>17.690303</td>\n",
       "      <td>3.474562</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000114</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>1.842560</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3.395878</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.360013</td>\n",
       "      <td>0.429366</td>\n",
       "      <td>2.085287</td>\n",
       "      <td>0.212607</td>\n",
       "      <td>3.610686</td>\n",
       "      <td>0.137334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.158710</td>\n",
       "      <td>0.248820</td>\n",
       "      <td>1.282190</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.592812</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1.357030</td>\n",
       "      <td>0.426691</td>\n",
       "      <td>2.020841</td>\n",
       "      <td>0.197245</td>\n",
       "      <td>3.431358</td>\n",
       "      <td>0.103488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1.154324</td>\n",
       "      <td>0.244887</td>\n",
       "      <td>3.097722</td>\n",
       "      <td>0.201381</td>\n",
       "      <td>6.761527</td>\n",
       "      <td>0.473908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>3.315756</td>\n",
       "      <td>2.183443</td>\n",
       "      <td>2.927501</td>\n",
       "      <td>2.330118</td>\n",
       "      <td>3.422077</td>\n",
       "      <td>2.060617</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1.779925</td>\n",
       "      <td>0.805979</td>\n",
       "      <td>2.532323</td>\n",
       "      <td>0.718383</td>\n",
       "      <td>4.327215</td>\n",
       "      <td>0.680561</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>4.678548</td>\n",
       "      <td>3.405710</td>\n",
       "      <td>6.994089</td>\n",
       "      <td>4.473998</td>\n",
       "      <td>13.186105</td>\n",
       "      <td>5.103808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prcp1       SM1     Prcp2       SM2      Prcp3       SM3  flood1  \\\n",
       "0    1.539606  0.590441  2.511432  0.474052   4.585283  0.484656       0   \n",
       "1    2.274125  1.249220  7.461011  2.159259  17.690303  3.474562       1   \n",
       "2    1.000114  0.106579  1.842560  0.010000   3.395878  0.010000       0   \n",
       "3    1.360013  0.429366  2.085287  0.212607   3.610686  0.137334       0   \n",
       "4    1.158710  0.248820  1.282190  0.010000   1.592812  0.010000       0   \n",
       "..        ...       ...       ...       ...        ...       ...     ...   \n",
       "360  1.357030  0.426691  2.020841  0.197245   3.431358  0.103488       0   \n",
       "361  1.154324  0.244887  3.097722  0.201381   6.761527  0.473908       0   \n",
       "362  3.315756  2.183443  2.927501  2.330118   3.422077  2.060617       1   \n",
       "363  1.779925  0.805979  2.532323  0.718383   4.327215  0.680561       0   \n",
       "364  4.678548  3.405710  6.994089  4.473998  13.186105  5.103808       1   \n",
       "\n",
       "     flood2  flood3  \n",
       "0         0       0  \n",
       "1         1       1  \n",
       "2         0       0  \n",
       "3         0       0  \n",
       "4         0       0  \n",
       "..      ...     ...  \n",
       "360       0       0  \n",
       "361       0       1  \n",
       "362       1       0  \n",
       "363       0       0  \n",
       "364       1       1  \n",
       "\n",
       "[365 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"datasetsrip - synthetic-prcp-SM-flood-data-v1.csv.csv\")\n",
    "data_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5d5a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Loss = 0.5859\n",
      "Epoch 2/10: Loss = 0.4774\n",
      "Epoch 3/10: Loss = 0.4136\n",
      "Epoch 4/10: Loss = 0.3423\n",
      "Epoch 5/10: Loss = 0.2900\n",
      "Epoch 6/10: Loss = 0.2511\n",
      "Epoch 7/10: Loss = 0.2238\n",
      "Epoch 8/10: Loss = 0.2183\n",
      "Epoch 9/10: Loss = 0.2193\n",
      "Epoch 10/10: Loss = 0.1871\n",
      "Test Accuracy: 0.9524\n"
     ]
    }
   ],
   "source": [
    "def test_1layer_RNN_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float),\n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long) \n",
    "            ))\n",
    "        \n",
    "    class RNN_GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(RNN_GCN, self).__init__()\n",
    "            self.rnn = nn.RNN(input_size=in_channels, hidden_size=16, num_layers=1, batch_first=True)\n",
    "            \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x, h = self.rnn(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        \n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RNN_GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75  \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_1layer_RNN_2layer_GCN(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4f945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Loss = 0.6490\n",
      "Epoch 2/20: Loss = 0.5531\n",
      "Epoch 3/20: Loss = 0.4532\n",
      "Epoch 4/20: Loss = 0.3563\n",
      "Epoch 5/20: Loss = 0.3001\n",
      "Epoch 6/20: Loss = 0.2608\n",
      "Epoch 7/20: Loss = 0.2241\n",
      "Epoch 8/20: Loss = 0.2049\n",
      "Epoch 9/20: Loss = 0.1810\n",
      "Epoch 10/20: Loss = 0.1747\n",
      "Epoch 11/20: Loss = 0.1645\n",
      "Epoch 12/20: Loss = 0.1504\n",
      "Epoch 13/20: Loss = 0.1515\n",
      "Epoch 14/20: Loss = 0.1353\n",
      "Epoch 15/20: Loss = 0.1342\n",
      "Epoch 16/20: Loss = 0.1320\n",
      "Epoch 17/20: Loss = 0.1207\n",
      "Epoch 18/20: Loss = 0.1185\n",
      "Epoch 19/20: Loss = 0.1101\n",
      "Epoch 20/20: Loss = 0.1065\n",
      "Test Accuracy: 0.9670\n"
     ]
    }
   ],
   "source": [
    "def test_1layer_RNN_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float),\n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long) \n",
    "            ))\n",
    "        \n",
    "    class RNN_GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(RNN_GCN, self).__init__()\n",
    "            self.rnn = nn.RNN(input_size=in_channels, hidden_size=16, num_layers=1, batch_first=True)\n",
    "            \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x, h = self.rnn(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        \n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RNN_GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75  \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_1layer_RNN_2layer_GCN(syn_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa6e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Loss = 0.5807\n",
      "Epoch 2/50: Loss = 0.4503\n",
      "Epoch 3/50: Loss = 0.3789\n",
      "Epoch 4/50: Loss = 0.3217\n",
      "Epoch 5/50: Loss = 0.2941\n",
      "Epoch 6/50: Loss = 0.2707\n",
      "Epoch 7/50: Loss = 0.2432\n",
      "Epoch 8/50: Loss = 0.2256\n",
      "Epoch 9/50: Loss = 0.2104\n",
      "Epoch 10/50: Loss = 0.1984\n",
      "Epoch 11/50: Loss = 0.1887\n",
      "Epoch 12/50: Loss = 0.1900\n",
      "Epoch 13/50: Loss = 0.1869\n",
      "Epoch 14/50: Loss = 0.1742\n",
      "Epoch 15/50: Loss = 0.1656\n",
      "Epoch 16/50: Loss = 0.1677\n",
      "Epoch 17/50: Loss = 0.1559\n",
      "Epoch 18/50: Loss = 0.1573\n",
      "Epoch 19/50: Loss = 0.1473\n",
      "Epoch 20/50: Loss = 0.1527\n",
      "Epoch 21/50: Loss = 0.1488\n",
      "Epoch 22/50: Loss = 0.1498\n",
      "Epoch 23/50: Loss = 0.1527\n",
      "Epoch 24/50: Loss = 0.1464\n",
      "Epoch 25/50: Loss = 0.1488\n",
      "Epoch 26/50: Loss = 0.1419\n",
      "Epoch 27/50: Loss = 0.1444\n",
      "Epoch 28/50: Loss = 0.1377\n",
      "Epoch 29/50: Loss = 0.1309\n",
      "Epoch 30/50: Loss = 0.1301\n",
      "Epoch 31/50: Loss = 0.1294\n",
      "Epoch 32/50: Loss = 0.1263\n",
      "Epoch 33/50: Loss = 0.1220\n",
      "Epoch 34/50: Loss = 0.1262\n",
      "Epoch 35/50: Loss = 0.1136\n",
      "Epoch 36/50: Loss = 0.1237\n",
      "Epoch 37/50: Loss = 0.1308\n",
      "Epoch 38/50: Loss = 0.1125\n",
      "Epoch 39/50: Loss = 0.1118\n",
      "Epoch 40/50: Loss = 0.1157\n",
      "Epoch 41/50: Loss = 0.1159\n",
      "Epoch 42/50: Loss = 0.1125\n",
      "Epoch 43/50: Loss = 0.1107\n",
      "Epoch 44/50: Loss = 0.1092\n",
      "Epoch 45/50: Loss = 0.1118\n",
      "Epoch 46/50: Loss = 0.1032\n",
      "Epoch 47/50: Loss = 0.1218\n",
      "Epoch 48/50: Loss = 0.1040\n",
      "Epoch 49/50: Loss = 0.1054\n",
      "Epoch 50/50: Loss = 0.0951\n",
      "Test Accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "def test_1layer_RNN_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float),\n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long) \n",
    "            ))\n",
    "        \n",
    "    class RNN_GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(RNN_GCN, self).__init__()\n",
    "            self.rnn = nn.RNN(input_size=in_channels, hidden_size=16, num_layers=1, batch_first=True)\n",
    "            \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x, h = self.rnn(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        \n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RNN_GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75  \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_1layer_RNN_2layer_GCN(syn_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c43676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Loss = 0.5588\n",
      "Epoch 2/100: Loss = 0.4437\n",
      "Epoch 3/100: Loss = 0.3651\n",
      "Epoch 4/100: Loss = 0.3095\n",
      "Epoch 5/100: Loss = 0.2792\n",
      "Epoch 6/100: Loss = 0.2482\n",
      "Epoch 7/100: Loss = 0.2381\n",
      "Epoch 8/100: Loss = 0.2259\n",
      "Epoch 9/100: Loss = 0.2020\n",
      "Epoch 10/100: Loss = 0.1888\n",
      "Epoch 11/100: Loss = 0.1810\n",
      "Epoch 12/100: Loss = 0.1732\n",
      "Epoch 13/100: Loss = 0.1710\n",
      "Epoch 14/100: Loss = 0.1638\n",
      "Epoch 15/100: Loss = 0.1579\n",
      "Epoch 16/100: Loss = 0.1525\n",
      "Epoch 17/100: Loss = 0.1440\n",
      "Epoch 18/100: Loss = 0.1371\n",
      "Epoch 19/100: Loss = 0.1377\n",
      "Epoch 20/100: Loss = 0.1395\n",
      "Epoch 21/100: Loss = 0.1270\n",
      "Epoch 22/100: Loss = 0.1237\n",
      "Epoch 23/100: Loss = 0.1111\n",
      "Epoch 24/100: Loss = 0.1149\n",
      "Epoch 25/100: Loss = 0.1115\n",
      "Epoch 26/100: Loss = 0.1044\n",
      "Epoch 27/100: Loss = 0.1098\n",
      "Epoch 28/100: Loss = 0.1023\n",
      "Epoch 29/100: Loss = 0.1076\n",
      "Epoch 30/100: Loss = 0.0956\n",
      "Epoch 31/100: Loss = 0.0939\n",
      "Epoch 32/100: Loss = 0.0904\n",
      "Epoch 33/100: Loss = 0.0894\n",
      "Epoch 34/100: Loss = 0.0827\n",
      "Epoch 35/100: Loss = 0.0799\n",
      "Epoch 36/100: Loss = 0.0753\n",
      "Epoch 37/100: Loss = 0.0888\n",
      "Epoch 38/100: Loss = 0.0725\n",
      "Epoch 39/100: Loss = 0.0762\n",
      "Epoch 40/100: Loss = 0.0760\n",
      "Epoch 41/100: Loss = 0.0809\n",
      "Epoch 42/100: Loss = 0.0746\n",
      "Epoch 43/100: Loss = 0.0614\n",
      "Epoch 44/100: Loss = 0.0695\n",
      "Epoch 45/100: Loss = 0.0664\n",
      "Epoch 46/100: Loss = 0.0630\n",
      "Epoch 47/100: Loss = 0.0633\n",
      "Epoch 48/100: Loss = 0.0616\n",
      "Epoch 49/100: Loss = 0.0617\n",
      "Epoch 50/100: Loss = 0.0591\n",
      "Epoch 51/100: Loss = 0.0578\n",
      "Epoch 52/100: Loss = 0.0584\n",
      "Epoch 53/100: Loss = 0.0592\n",
      "Epoch 54/100: Loss = 0.0575\n",
      "Epoch 55/100: Loss = 0.0513\n",
      "Epoch 56/100: Loss = 0.0525\n",
      "Epoch 57/100: Loss = 0.0499\n",
      "Epoch 58/100: Loss = 0.0489\n",
      "Epoch 59/100: Loss = 0.0519\n",
      "Epoch 60/100: Loss = 0.0521\n",
      "Epoch 61/100: Loss = 0.0556\n",
      "Epoch 62/100: Loss = 0.0498\n",
      "Epoch 63/100: Loss = 0.0463\n",
      "Epoch 64/100: Loss = 0.0472\n",
      "Epoch 65/100: Loss = 0.0475\n",
      "Epoch 66/100: Loss = 0.0443\n",
      "Epoch 67/100: Loss = 0.0468\n",
      "Epoch 68/100: Loss = 0.0712\n",
      "Epoch 69/100: Loss = 0.0626\n",
      "Epoch 70/100: Loss = 0.0535\n",
      "Epoch 71/100: Loss = 0.0507\n",
      "Epoch 72/100: Loss = 0.0432\n",
      "Epoch 73/100: Loss = 0.0431\n",
      "Epoch 74/100: Loss = 0.0460\n",
      "Epoch 75/100: Loss = 0.0463\n",
      "Epoch 76/100: Loss = 0.0382\n",
      "Epoch 77/100: Loss = 0.0434\n",
      "Epoch 78/100: Loss = 0.0461\n",
      "Epoch 79/100: Loss = 0.0405\n",
      "Epoch 80/100: Loss = 0.0407\n",
      "Epoch 81/100: Loss = 0.0356\n",
      "Epoch 82/100: Loss = 0.0384\n",
      "Epoch 83/100: Loss = 0.0368\n",
      "Epoch 84/100: Loss = 0.0389\n",
      "Epoch 85/100: Loss = 0.0412\n",
      "Epoch 86/100: Loss = 0.0435\n",
      "Epoch 87/100: Loss = 0.0411\n",
      "Epoch 88/100: Loss = 0.0320\n",
      "Epoch 89/100: Loss = 0.0322\n",
      "Epoch 90/100: Loss = 0.0316\n",
      "Epoch 91/100: Loss = 0.0389\n",
      "Epoch 92/100: Loss = 0.0375\n",
      "Epoch 93/100: Loss = 0.0328\n",
      "Epoch 94/100: Loss = 0.0332\n",
      "Epoch 95/100: Loss = 0.0375\n",
      "Epoch 96/100: Loss = 0.0383\n",
      "Epoch 97/100: Loss = 0.0337\n",
      "Epoch 98/100: Loss = 0.0366\n",
      "Epoch 99/100: Loss = 0.0347\n",
      "Epoch 100/100: Loss = 0.0340\n",
      "Test Accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "def test_1layer_RNN_2layer_GCN(df, window = 3):\n",
    "    df_list = [df[i:i+window] for i in range(0,df.shape[0]-window+1,1)]\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict['x'] = [df_list[i].iloc[:,0:6] for i in range(len(df_list))]\n",
    "    df_dict['y'] = [df_list[i].iloc[-1,-3:] for i in range(len(df_list))]\n",
    "\n",
    "    \n",
    "    def create_node_features(df_x):\n",
    "        Prcp1 = df_x['Prcp1'].values\n",
    "        Prcp2 = df_x['Prcp2'].values\n",
    "        Prcp3 = df_x['Prcp3'].values\n",
    "        SM1 = df_x['SM1'].values\n",
    "        SM2 = df_x['SM2'].values\n",
    "        SM3 = df_x['SM3'].values\n",
    "        node_1 = np.hstack((Prcp1, SM1))\n",
    "        node_2 = np.hstack((Prcp2, SM2))\n",
    "        node_3 = np.hstack((Prcp3, SM3))\n",
    "        node_features = np.vstack((node_1, node_2, node_3))\n",
    "        return node_features\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(df_dict['x'])):\n",
    "        data_list.append(Data(\n",
    "            x=torch.tensor(create_node_features(df_dict['x'][i]), dtype=torch.float), \n",
    "            y=torch.tensor(df_dict['y'][i].values, dtype=torch.float),\n",
    "            edge_index=torch.tensor([[0,2], [1,1]], dtype=torch.long) \n",
    "            ))\n",
    "        \n",
    "    class RNN_GCN(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super(RNN_GCN, self).__init__()\n",
    "            self.rnn = nn.RNN(input_size=in_channels, hidden_size=16, num_layers=1, batch_first=True)\n",
    "            \n",
    "            self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x, h = self.rnn(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        \n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RNN_GCN(in_channels=2*window, out_channels=1).to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_ratio = 0.75  \n",
    "    train_size = int(train_ratio * len(data_list))\n",
    "    train_data = data_list[:train_size]\n",
    "    test_data = data_list[train_size:]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Loss = {total_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index).squeeze()\n",
    "            predicted_labels = (out > 0.5).long()\n",
    "            correct += (predicted_labels == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_1layer_RNN_2layer_GCN(syn_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9245b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
